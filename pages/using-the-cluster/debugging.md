---
layout: single
title: Debugging Jobs & Viewing Logs
permalink: /using-the-cluster/debugging/
excerpt: "A guide to diagnosing and fixing issues with your Slurm jobs."
---

## Introduction to Debugging

When a batch job fails, it can be frustrating. However, Slurm and the Linux environment provide powerful tools to help you diagnose the problem. The key to effective debugging is understanding where to look for information and how to interpret it.

This guide covers the most common steps for debugging a failed or misbehaving job on the cluster.

## 1. Check Your Job's Log Files

The first place you should always look is the output and error files generated by your job. By default, Slurm writes all standard output (`stdout`) and standard error (`stderr`) from your script to these files.

In your submission script, you specify their names with `#SBATCH`:
```bash
#SBATCH --output=my_job_%j.out
#SBATCH --error=my_job_%j.err
```
- **`my_job_%j.out`**: This file captures everything your script would normally print to the terminal. Use `echo` statements in your script to trace its progress.
- **`my_job_%j.err`**: This file captures all error messages. If your program crashes, the error message will almost always be here.

**Always check the `.err` file first.** Even if it's empty, that's useful information.

## 2. Use `sacct` to Get Job History

The `sacct` command is essential for post-mortem analysis of your jobs. It can tell you why a job failed and how many resources it actually used.

To check a specific job:
```bash
sacct -j <JOB_ID> --format=JobID,JobName,State,ExitCode,MaxRSS,Elapsed
```

Key fields to examine:
- **`State`**: This will tell you the final status. Common failure states are `FAILED`, `OUT_OF_MEMORY`, and `TIMEOUT`.
    - `FAILED`: The job script exited with a non-zero exit code. The reason is likely in your `.err` file.
    - `OUT_OF_MEMORY`: The job exceeded its requested memory (`--mem`). Slurm killed it.
    - `TIMEOUT`: The job ran longer than its requested time (`--time`). Slurm killed it.
    - `CANCELLED`: The job was cancelled, either by you or an administrator.
- **`ExitCode`**: This shows the exit code from your script.
    - `0:0`: Success.
    - `1:0` (or any non-zero code): Failure. The specific code can sometimes indicate the type of error. For example, `127` often means "command not found".
- **`MaxRSS`**: The maximum amount of memory the job used. This is incredibly useful for tuning your next submission. If your job failed with an `OUT_OF_MEMORY` error, you know you need to request more than this value. If it succeeded, you can use this value (plus a small buffer) to request memory more accurately in the future.

## 3. Common Failure Scenarios and Solutions

| Symptom | Likely Cause & Solution |
|:---|:---|
| Job fails immediately. `.err` file shows `...: command not found`. | **Cause:** The program you're trying to run isn't in your `PATH`. This usually means you forgot to load a software module.<br>**Solution:** Add the appropriate `module load <module_name>` command to your Slurm script before you call the program. |
| `sacct` shows job `State` is `OUT_OF_MEMORY`. | **Cause:** Your job used more memory than you requested with `--mem`.<br>**Solution:** Check the `MaxRSS` value in `sacct` for the failed job. Resubmit the job with a higher `--mem` value (e.g., `MaxRSS` + 20% buffer). |
| `sacct` shows job `State` is `TIMEOUT`. | **Cause:** Your job ran for longer than the wall-clock time you requested with `--time`.<br>**Solution:** Estimate a more realistic runtime and increase the `--time` value in your submission script. |
| Job is `PENDING` for a long time with reason `(Resources)`. | **Cause:** The cluster does not currently have the resources (CPUs, memory, GPUs) that you requested. You might be asking for too many resources, or the cluster is just very busy.<br>**Solution:** Double-check your resource requests to ensure they are reasonable. If they are, you may just need to wait. You can also try requesting fewer resources if your job can run with less. |
| Python script fails with `ModuleNotFoundError: No module named '...'`. | **Cause:** The required Python package is not installed in the Python environment your job is using.<br>**Solution:** Ensure you have loaded the correct Python module (e.g., `module load anaconda`) and that the package is installed. For custom environments, make sure you activate the correct conda/venv environment in your script. Using containers is an excellent way to avoid this problem. |
| Script fails with `No such file or directory`. | **Cause:** The script is trying to access a file using a path that doesn't exist from the compute node's perspective. This often happens with relative paths.<br>**Solution:** Use absolute paths (e.g., `/home/$USER/my_project/data.txt`) for your files. Remember that your job's working directory is where you ran `sbatch` from, unless you `cd` elsewhere in your script. |

## 4. Use an Interactive Session for Hands-On Debugging

When log files aren't enough, the best way to debug is to get a "hands-on" session on a compute node. This allows you to run your commands one by one and see the errors in real-time.

1.  **Request an interactive job with the same resources as your failed job.** This is critical for reproducing the environment and the error.
    ```bash
    # Example: your failed job requested 4 CPUs and 8G of memory for 1 hour
    srun --nodes=1 --cpus-per-task=4 --mem=8G --time=01:00:00 --pty /bin/bash
    ```

2.  **Once on the compute node, load the same modules** your batch script was using.
    ```bash
    module load python anaconda
    ```

3.  **Run the commands from your script line by line.** Copy and paste each command into the interactive terminal. This will help you pinpoint exactly which command is failing and see the immediate error message.

This method is the most effective way to solve complex issues that are not obvious from the log files. For more details, see the Interactive Sessions page.
